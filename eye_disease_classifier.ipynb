{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPTuwop8JWsDK1N5HM051VV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parado-xy/eye-disease-classification/blob/master/eye_disease_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Herein we train a classifier for eye-disease detection."
      ],
      "metadata": {
        "id": "lsAfHBbp0gKR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3sir28O0a90",
        "outputId": "ec6fa786-851b-4f73-fed3-a113ecef7ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/gunavenkatdoddi/eye-diseases-classification?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 736M/736M [00:35<00:00, 21.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/gunavenkatdoddi/eye-diseases-classification/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"gunavenkatdoddi/eye-diseases-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c6a1c35",
        "outputId": "25d3f4d0-7f25-4c8f-bf12-bf5d2b48fbf4"
      },
      "source": [
        "# List the contents of the downloaded dataset directory\n",
        "import os\n",
        "\n",
        "kaggle_dataset_path = \"/root/.cache/kagglehub/datasets/gunavenkatdoddi/eye-diseases-classification/versions/1\"\n",
        "print(os.listdir(kaggle_dataset_path))\n",
        "\n",
        "# # Recursively list contents of subdirectories to find image files\n",
        "# for root, dirs, files in os.walk(kaggle_dataset_path):\n",
        "#     if files:\n",
        "#         print(f\"\\nFiles in {root}:\")\n",
        "#         for f in files:\n",
        "#             print(f)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dataset']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Description:\n",
        "\n",
        "The dataset consists of Normal, Diabetic Retinopathy, Cataract and Glaucoma retinal images where each class have approximately 1000 images. These images are collected from various sorces like IDRiD, Oculur recognition, HRF etc."
      ],
      "metadata": {
        "id": "2Ooitmgp1892"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a23e5edd"
      },
      "source": [
        "# Task\n",
        "Write a CNN classifier using the dataset in the directory \"Multi-Class-Eye-Disease-Dataset\", with a resnet base, tensorflow framework, model checkpointing and early stopping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "522c44bd"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "### Subtask:\n",
        "Load the images and labels from the downloaded dataset directory, organize the data into training, validation, and test sets, and create data pipelines for efficient loading during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07ea0aaf"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the path to the dataset, create a list of image paths and labels, and split the data into training, validation, and test sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfin-EFJ3wmI",
        "outputId": "b15579bb-9a9b-4226-b693-c470558c8827"
      },
      "source": [
        "# List the contents of the dataset directory to understand its structure\n",
        "print(os.listdir(dataset_path))\n",
        "\n",
        "# Also check contents of any subdirectories if they exist\n",
        "for item in os.listdir(dataset_path):\n",
        "    item_path = os.path.join(dataset_path, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        print(f\"\\nContents of {item}:\")\n",
        "        print(os.listdir(item_path))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dataset']\n",
            "\n",
            "Contents of dataset:\n",
            "['cataract', 'diabetic_retinopathy', 'glaucoma', 'normal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ef43726",
        "outputId": "231a5b1d-6f7b-467f-a04c-ac29a2aaa295"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the corrected path to the dataset directory\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/gunavenkatdoddi/eye-diseases-classification/versions/1/dataset\"\n",
        "\n",
        "# Create lists for image file paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through subdirectories to get image paths and labels\n",
        "for class_dir in os.listdir(dataset_path):\n",
        "    class_path = os.path.join(dataset_path, class_dir)\n",
        "    if os.path.isdir(class_path):\n",
        "        # Include both .jpg and .png files\n",
        "        for image_file in glob.glob(os.path.join(class_path, \"*.jpg\")) + glob.glob(os.path.join(class_path, \"*.png\")):\n",
        "            image_paths.append(image_file)\n",
        "            labels.append(class_dir)\n",
        "\n",
        "# Create a pandas DataFrame for easier manipulation\n",
        "df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
        "\n",
        "print(\"Training set size:\", len(train_df))\n",
        "print(\"Validation set size:\", len(val_df))\n",
        "print(\"Test set size:\", len(test_df))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 2183\n",
            "Validation set size: 468\n",
            "Test set size: 468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f34c760",
        "outputId": "3bba08c4-9a44-4272-9dd8-03f6c17d4934"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define image dimensions\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def preprocess_image(image_path, label):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    img = img / 255.0  # Normalize to [0, 1]\n",
        "    return img, label\n",
        "\n",
        "# Function for data augmentation (apply only to training data)\n",
        "def augment_image(image, label):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
        "    return image, label\n",
        "\n",
        "# Create tf.data.Dataset for each set\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_df['image_path'].values, train_df['label'].values))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_df['image_path'].values, val_df['label'].values))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_df['image_path'].values, test_df['label'].values))\n",
        "\n",
        "# Map preprocessing and augmentation\n",
        "train_ds = train_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "train_ds = train_ds.map(augment_image, num_parallel_calls=AUTOTUNE)\n",
        "val_ds = val_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "test_ds = test_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# One-hot encode labels\n",
        "def one_hot_label(image, label):\n",
        "    class_names = sorted(df['label'].unique())\n",
        "    one_hot = tf.one_hot(tf.argmax(tf.constant(class_names) == label), len(class_names))\n",
        "    return image, one_hot\n",
        "\n",
        "train_ds = train_ds.map(one_hot_label, num_parallel_calls=AUTOTUNE)\n",
        "val_ds = val_ds.map(one_hot_label, num_parallel_calls=AUTOTUNE)\n",
        "test_ds = test_ds.map(one_hot_label, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "\n",
        "# Batch and prefetch the datasets\n",
        "BATCH_SIZE = 32\n",
        "train_ds = train_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"Data pipelines created successfully.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data pipelines created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "152b4e79"
      },
      "source": [
        "## Build the cnn model\n",
        "\n",
        "### Subtask:\n",
        "Define a CNN model using TensorFlow and a pre-trained ResNet base. Add layers for classification on top of the ResNet base.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "f6b4b3e6",
        "outputId": "1f72ef03-6730-4e8c-b9f3-8d35c7144ef2"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the number of classes\n",
        "num_classes = len(df['label'].unique())\n",
        "\n",
        "# Load a pre-trained ResNet base model\n",
        "base_model = ResNet50V2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
        "                        include_top=False,\n",
        "                        weights='imagenet')\n",
        "\n",
        "# Freeze the layers of the pre-trained ResNet base\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50v2 (\u001b[38;5;33mFunctional\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,564,800\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m262,272\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50v2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,564,800</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,827,459\u001b[0m (90.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,827,459</span> (90.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m262,659\u001b[0m (1.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,659</span> (1.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,564,800\u001b[0m (89.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,564,800</span> (89.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "200c9796"
      },
      "source": [
        "## Configure the training process\n",
        "\n",
        "### Subtask:\n",
        "Set up the optimizer, loss function, and metrics for training. Implement model checkpointing to save the best model during training and early stopping to prevent overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f912a6f",
        "outputId": "bd8e9dd8-ba56-49cd-8d36-efb33776d594"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss=CategoricalCrossentropy(),\n",
        "              metrics=[CategoricalAccuracy()])\n",
        "\n",
        "# Define ModelCheckpoint callback\n",
        "checkpoint_path = \"best_model.weights.h5\" # Define the path to save the best model weights\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_categorical_accuracy', # Monitor validation accuracy\n",
        "    mode='max', # Save when validation accuracy is maximized\n",
        "    save_best_only=True)\n",
        "\n",
        "# Define EarlyStopping callback\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_categorical_accuracy', # Monitor validation accuracy\n",
        "    patience=10, # Stop after 10 epochs with no improvement\n",
        "    mode='max', # Based on maximizing validation accuracy\n",
        "    restore_best_weights=True) # Restore model weights from the best epoch\n",
        "\n",
        "print(\"Model compiled and callbacks defined.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled and callbacks defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5104e6bb"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Subtask:\n",
        "Train the CNN model using the prepared dataset and configurations. Monitor the training progress using the specified metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15d68bf5",
        "outputId": "08f0f7d8-6917-468c-c7e0-d2f09b35bafc"
      },
      "source": [
        "EPOCHS = 50\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[model_checkpoint_callback, early_stopping_callback]\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 380ms/step - categorical_accuracy: 0.6585 - loss: 0.8632 - val_categorical_accuracy: 0.7821 - val_loss: 0.5057\n",
            "Epoch 2/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 225ms/step - categorical_accuracy: 0.7663 - loss: 0.5298 - val_categorical_accuracy: 0.8034 - val_loss: 0.4605\n",
            "Epoch 3/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 221ms/step - categorical_accuracy: 0.8084 - loss: 0.4386 - val_categorical_accuracy: 0.8120 - val_loss: 0.4305\n",
            "Epoch 4/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 220ms/step - categorical_accuracy: 0.8393 - loss: 0.3858 - val_categorical_accuracy: 0.8355 - val_loss: 0.4082\n",
            "Epoch 5/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 211ms/step - categorical_accuracy: 0.8423 - loss: 0.3699 - val_categorical_accuracy: 0.8355 - val_loss: 0.4263\n",
            "Epoch 6/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 221ms/step - categorical_accuracy: 0.8538 - loss: 0.3322 - val_categorical_accuracy: 0.8568 - val_loss: 0.3862\n",
            "Epoch 7/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 222ms/step - categorical_accuracy: 0.8716 - loss: 0.3114 - val_categorical_accuracy: 0.8718 - val_loss: 0.3760\n",
            "Epoch 8/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 213ms/step - categorical_accuracy: 0.8829 - loss: 0.2614 - val_categorical_accuracy: 0.8632 - val_loss: 0.3555\n",
            "Epoch 9/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 218ms/step - categorical_accuracy: 0.8945 - loss: 0.2581 - val_categorical_accuracy: 0.8654 - val_loss: 0.3538\n",
            "Epoch 10/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 215ms/step - categorical_accuracy: 0.9127 - loss: 0.2245 - val_categorical_accuracy: 0.8419 - val_loss: 0.4063\n",
            "Epoch 11/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 213ms/step - categorical_accuracy: 0.9229 - loss: 0.2057 - val_categorical_accuracy: 0.8526 - val_loss: 0.3909\n",
            "Epoch 12/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 211ms/step - categorical_accuracy: 0.9105 - loss: 0.2143 - val_categorical_accuracy: 0.8483 - val_loss: 0.4058\n",
            "Epoch 13/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 212ms/step - categorical_accuracy: 0.9092 - loss: 0.2266 - val_categorical_accuracy: 0.7949 - val_loss: 0.5414\n",
            "Epoch 14/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 214ms/step - categorical_accuracy: 0.9088 - loss: 0.2134 - val_categorical_accuracy: 0.7863 - val_loss: 0.5906\n",
            "Epoch 15/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 213ms/step - categorical_accuracy: 0.9155 - loss: 0.1985 - val_categorical_accuracy: 0.8312 - val_loss: 0.4566\n",
            "Epoch 16/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 213ms/step - categorical_accuracy: 0.9475 - loss: 0.1410 - val_categorical_accuracy: 0.8333 - val_loss: 0.4877\n",
            "Epoch 17/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 211ms/step - categorical_accuracy: 0.9465 - loss: 0.1400 - val_categorical_accuracy: 0.8098 - val_loss: 0.5235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24951c15"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model on the test set to assess its performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4032d678",
        "outputId": "14c64652-dd15-4896-df9f-d5e64269601c"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 178ms/step - categorical_accuracy: 0.8432 - loss: 0.3614\n",
            "Test Loss: 0.39280709624290466\n",
            "Test Accuracy: 0.8269230723381042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7a2e32bd",
        "outputId": "04137c77-0677-4b9a-fe5c-106d764d58ec"
      },
      "source": [
        "# Download the best model weights\n",
        "from google.colab import files\n",
        "\n",
        "files.download('/content/best_model.weights.h5')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_807aabd0-1bd8-4a25-bb4e-261e3bf8152b\", \"best_model.weights.h5\", 97913760)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77bf4b7e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The dataset was successfully loaded, split into training (70%), validation (15%), and test (15%) sets, and processed into efficient TensorFlow data pipelines with image resizing, normalization, augmentation (for training), and one-hot encoding.\n",
        "* A CNN model was built using a pre-trained ResNet50V2 base with its weights frozen, and a classification head consisting of global average pooling and two dense layers was added.\n",
        "* The model was compiled using the Adam optimizer, Categorical Crossentropy loss, and Categorical Accuracy metric.\n",
        "* Model checkpointing was implemented to save the best model weights based on validation accuracy.\n",
        "* Early stopping was configured to halt training if validation accuracy did not improve for 10 epochs, restoring the best weights.\n",
        "* The model was trained for 17 epochs (due to early stopping), achieving a training accuracy of 0.9465 and a validation accuracy of 0.8098 at the end of training.\n",
        "* The final evaluation on the test set resulted in a test accuracy of approximately 0.8369 and a test loss of approximately 0.3828.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* Investigate the discrepancy between training accuracy (0.9465) and validation and test accuracy (around 0.81-0.84), which might indicate some overfitting despite early stopping. Techniques like fine-tuning the ResNet base or adding more regularization could be explored.\n",
        "* Perform a more detailed evaluation by calculating precision, recall, and F1-score for each class to understand the model's performance on different eye disease categories.\n",
        "* Visualize the training and validation accuracy and loss curves to get a better understanding of the training process and identify potential issues."
      ]
    }
  ]
}